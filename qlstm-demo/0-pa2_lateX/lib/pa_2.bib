@InProceedings{Weigold2022,
  author        = {Weigold, Manuela and Barzen, Johanna and Leymann, Frank and Salm, Marie},
  booktitle     = {Proceedings of the 27th Conference on Pattern Languages of Programs},
  title         = {Data Encoding Patterns for Quantum Computing},
  year          = {2022},
  address       = {USA},
  pages         = {1--11},
  publisher     = {The Hillside Group},
  series        = {PLoP '20},
  abstract      = {Quantum computers have the potential to solve certain problems faster than classical computers. However, loading data into a quantum computer is not trivial. To load the data, it must be encoded in quantum bits (qubits). There are several ways how qubits can represent the data and, thus, multiple data encodings are possible. Both the data itself and the chosen encoding influence the runtime of the loading process. In the worst case, loading requires exponential time. This is critical because quantum algorithms that promise a speed-up assume that loading data can be done faster, in logarithmic or linear time. To outline abstract knowledge about encodings and the consequences of choosing a particular data encoding, we present three common encodings as patterns. Especially in complex domains like quantum computing, patterns can contribute to making this new technology and its broad potential accessible to users with different backgrounds. In particular, they facilitate the development of quantum applications for software developers.},
  articleno     = {2},
  comment-jonas = {test wuhu},
  file          = {:papers/Weigold2022.pdf:PDF},
  groups        = {Data Encoding (Quantum)},
  isbn          = {9781941652169},
  keywords      = {quantum algorithms, quantum computing, speed-up, patterns, data encoding},
  location      = {Virtual Event},
  numpages      = {11},
}

@Article{Schuld2021,
  author    = {Schuld, Maria and Sweke, Ryan and Meyer, Johannes Jakob},
  journal   = {Phys. Rev. A},
  title     = {Effect of data encoding on the expressive power of variational quantum-machine-learning models},
  year      = {2021},
  month     = {Mar},
  pages     = {032430},
  volume    = {103},
  abstract  = {Quantum computers can be used for supervised learning by treating parametrized quantum circuits as models that map data inputs to predictions. While a lot of work has been done to investigate the practical implications of this approach, many important theoretical properties of these models remain unknown. Here, we investigate how the strategy with which data are encoded into the model influences the expressive power of parametrized quantum circuits as function approximators. We show that one can naturally write a quantum model as a partial Fourier series in the data, where the accessible frequencies are determined by the nature of the data-encoding gates in the circuit. By repeating simple data-encoding gates multiple times, quantum models can access increasingly rich frequency spectra. We show that there exist quantum models which can realize all possible sets of Fourier coefficients, and therefore, if the accessible frequency spectrum is asymptotically rich enough, such models are universal function approximators.},
  doi       = {10.1103/PhysRevA.103.032430},
  file      = {:papers/Schuld2021.pdf:PDF},
  groups    = {Data Encoding (Quantum)},
  issue     = {3},
  numpages  = {12},
  publisher = {American Physical Society},
  url       = {https://link.aps.org/doi/10.1103/PhysRevA.103.032430},
}

@InProceedings{Weigold2021,
  author    = {Weigold, Manuela and Barzen, Johanna and Leymann, Frank and Salm, Marie},
  booktitle = {2021 IEEE 18th International Conference on Software Architecture Companion (ICSA-C)},
  title     = {Expanding Data Encoding Patterns For Quantum Algorithms},
  year      = {2021},
  address   = {Stuttgart, Germany},
  pages     = {95-101},
  publisher = {IEEE},
  doi       = {10.1109/ICSA-C52384.2021.00025},
  file      = {:papers/Weigold2021.pdf:PDF},
  groups    = {Data Encoding (Quantum)},
}

@Article{BravoPrieto2021,
  author    = {Carlos Bravo-Prieto},
  journal   = {Machine Learning: Science and Technology},
  title     = {Quantum autoencoders with enhanced data encoding},
  year      = {2021},
  month     = {jul},
  number    = {3},
  pages     = {035028},
  volume    = {2},
  abstract  = {We present the enhanced feature quantum autoencoder, or EF-QAE, a variational quantum algorithm capable of compressing quantum states of different models with higher fidelity. The key idea of the algorithm is to define a parameterized quantum circuit that depends upon adjustable parameters and a feature vector that characterizes such a model. We assess the validity of the method in simulations by compressing ground states of the Ising model and classical handwritten digits. The results show that EF-QAE improves the performance compared to the standard quantum autoencoder using the same amount of quantum resources, but at the expense of additional classical optimization. Therefore, EF-QAE makes the task of compressing quantum information better suited to be implemented in near-term quantum devices.},
  doi       = {10.1088/2632-2153/ac0616},
  file      = {:papers/BravoPrieto2021.pdf:PDF},
  groups    = {Data Encoding (Quantum)},
  publisher = {IOP Publishing},
  url       = {https://dx.doi.org/10.1088/2632-2153/ac0616},
}

@Article{Weigold2021a,
  author   = {Weigold, Manuela and Barzen, Johanna and Leymann, Frank and Salm, Marie},
  journal  = {IET Quantum Communication},
  title    = {Encoding patterns for quantum algorithms},
  year     = {2021},
  month    = dec,
  number   = {4},
  pages    = {141-152},
  volume   = {2},
  abstract = {Abstract As quantum computers are based on the laws of quantum mechanics, they are capable of solving certain problems faster than their classical counterparts. However, quantum algorithms with a theoretical speed-up often assume that data can be loaded efficiently. In general, the runtime complexity of the loading routine depends on (i) the data encoding that defines how the data is represented by the state of the quantum computer and (ii) the data itself. In some cases, loading the data requires at least exponential time that destroys a potential speed-up. And especially for the first generation of devices that are currently available, the resources (qubits and operations) needed to encode the data are limited. In this work, we, therefore, present six patterns that describe how data is handled by quantum computers.},
  doi      = {https://doi.org/10.1049/qtc2.12032},
  file     = {:papers/Weigold2021a.pdf:PDF},
  groups   = {Data Encoding (Quantum)},
  keywords = {computational complexity, quantum computing techniques, quantum computing, quantum entanglement},
  url      = {https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/qtc2.12032},
}

@Article{Abbas2021,
  author   = {Abbas, Amira and Sutter, David and Zoufal, Christa and Lucchi, Aurelien and Figalli, Alessio and Woerner, Stefan},
  journal  = {Nature Computational Science},
  title    = {The power of quantum neural networks},
  year     = {2021},
  issn     = {2662-8457},
  month    = jun,
  number   = {6},
  pages    = {403--409},
  volume   = {1},
  abstract = {It is unknown whether near-term quantum computers are advantageous for machine learning tasks. In this work we address this question by trying to understand how powerful and trainable quantum machine learning models are in relation to popular classical neural networks. We propose the effective dimension--a measure that captures these qualities--and prove that it can be used to assess any statistical model’s ability to generalize on new data. Crucially, the effective dimension is a data-dependent measure that depends on the Fisher information, which allows us to gauge the ability of a model to train. We demonstrate numerically that a class of quantum neural networks is able to achieve a considerably better effective dimension than comparable feedforward networks and train faster, suggesting an advantage for quantum machine learning, which we verify on real quantum hardware.},
  doi      = {10.1038/s43588-021-00084-1},
  file     = {:papers/Abbas2021.pdf:PDF},
  ranking  = {rank5},
  refid    = {Abbas2021},
  url      = {https://doi.org/10.1038/s43588-021-00084-1},
}

@Article{Cerezo2022,
  author   = {Cerezo, M. and Verdon, Guillaume and Huang, Hsin-Yuan and Cincio, Lukasz and Coles, Patrick J.},
  journal  = {Nature Computational Science},
  title    = {Challenges and opportunities in quantum machine learning},
  year     = {2022},
  issn     = {2662-8457},
  month    = sep,
  number   = {9},
  pages    = {567--576},
  volume   = {2},
  abstract = {At the intersection of machine learning and quantum computing, quantum machine learning has the potential of accelerating data analysis, especially for quantum data, with applications for quantum materials, biochemistry and high-energy physics. Nevertheless, challenges remain regarding the trainability of quantum machine learning models. Here we review current methods and applications for quantum machine learning. We highlight differences between quantum and classical machine learning, with a focus on quantum neural networks and quantum deep learning. Finally, we discuss opportunities for quantum advantage with quantum machine learning.},
  doi      = {10.1038/s43588-022-00311-3},
  file     = {:papers/Cerezo2022.pdf:PDF},
  refid    = {Cerezo2022},
  url      = {https://doi.org/10.1038/s43588-022-00311-3},
}

@Book{Schuld2018,
  author    = {Schuld, Maria and Petruccione, Francesco},
  publisher = {Springer Cham},
  title     = {Supervised Learning with Quantum Computers},
  year      = {2018},
  address   = {Springer Nature Switzerland},
  edition   = {1},
  isbn      = {9783319964249},
  month     = sep,
  doi       = {https://doi.org/10.1007/978-3-319-96424-9},
  file      = {:papers/Schuld2018.pdf:PDF},
  ppn_gvk   = {1688304193},
}

@Online{Qiskit,
  url     = {https://learn.qiskit.org/course/machine-learning/data-encoding},
  urldate = {2023-09-14},
  author  = {Qiskit},
  groups  = {Data Encoding (Quantum)},
  title   = {Data Encoding},
  year    = {o. J.},
}

@Article{LaRose2020,
  author    = {LaRose, Ryan and Coyle, Brian},
  journal   = {Phys. Rev. A},
  title     = {Robust data encodings for quantum classifiers},
  year      = {2020},
  month     = {Sep},
  pages     = {403--409},
  volume    = {102},
  doi       = {10.1103/PhysRevA.102.032420},
  file      = {:papers/larose2020.pdf:PDF},
  groups    = {Data Encoding (Quantum)},
  issue     = {3},
  numpages  = {24},
  publisher = {American Physical Society},
  url       = {https://link.aps.org/doi/10.1103/PhysRevA.102.032420},
}

@Article{SierraSosa2023,
  author   = {Sierra-Sosa, Daniel and Pal, Soham and Telahun, Michael},
  journal  = {Quantum Information Processing},
  title    = {Data rotation and its influence on quantum encoding},
  year     = {2023},
  issn     = {1573-1332},
  number   = {1},
  pages    = {89},
  volume   = {22},
  abstract = {Parametric quantum machine learning (QML) has been vastly studied over the last several years. These algorithms rely on hybrid implementations, where quantum methods define the models, and the parameters are update on classical devices. The encoding of classical data into quantum states within the Hilbert space is fundamental to training these hybrid models; this can be achieved in a number of ways. In this work, we focus on two of these methods, amplitude encoding and encoding via a second-order Pauli feature map. We compared their performances across two near-term QML models, quantum support vector classifier and variational quantum classifier. We found that amplitude encoding is significantly resilient to classical transformations of data. This work additionally introduces the concept of a rotation, applied to classical data as a preprocessing step. In our results, we observe that other encoding methods can significantly benefit from certain Cartesian rotations of the data. We expand this rotation to a larger $${n-D}$$dataset and show the method’s performance.},
  doi      = {10.1007/s11128-023-03837-1},
  file     = {:papers/SierraSosa2023.pdf:PDF},
  groups   = {Data Encoding (Quantum)},
  refid    = {Sierra-Sosa2023},
  url      = {https://doi.org/10.1007/s11128-023-03837-1},
}

@Article{Majji2023,
  author  = {Majji, Sathwik Reddy and Chalumuri, Avinash and Manoj, B. S.},
  journal = {IEEE Sensors Letters},
  title   = {Quantum Approach to Image Data Encoding and Compression},
  year    = {2023},
  number  = {2},
  pages   = {1-4},
  volume  = {7},
  doi     = {10.1109/LSENS.2023.3239749},
  file    = {:papers/Majji2023.pdf:PDF},
  groups  = {Data Encoding (Quantum)},
}

@Article{Kim2023,
  author   = {Kim, Youngseok and Eddins, Andrew and Anand, Sajant and Wei, Ken Xuan and van den Berg, Ewout and Rosenblatt, Sami and Nayfeh, Hasan and Wu, Yantao and Zaletel, Michael and Temme, Kristan and Kandala, Abhinav},
  journal  = {Nature},
  title    = {Evidence for the utility of quantum computing before fault tolerance},
  year     = {2023},
  issn     = {1476-4687},
  number   = {7965},
  pages    = {500--505},
  volume   = {618},
  abstract = {Quantum computing promises to offer substantial speed-ups over its classical counterpart for certain problems. However, the greatest impediment to realizing its full potential is noise that is inherent to these systems. The widely accepted solution to this challenge is the implementation of fault-tolerant quantum circuits, which is out of reach for current processors. Here we report experiments on a noisy 127-qubit processor and demonstrate the measurement of accurate expectation values for circuit volumes at a scale beyond brute-force classical computation. We argue that this represents evidence for the utility of quantum computing in a pre-fault-tolerant era. These experimental results are enabled by advances in the coherence and calibration of a superconducting processor at this scale and the ability to characterize1 and controllably manipulate noise across such a large device. We establish the accuracy of the measured expectation values by comparing them with the output of exactly verifiable circuits. In the regime of strong entanglement, the quantum computer provides correct results for which leading classical approximations such as pure-state-based 1D (matrix product states, MPS) and 2D (isometric tensor network states, isoTNS) tensor network methods2,3 break down. These experiments demonstrate a foundational tool for the realization of near-term quantum applications4,5.},
  doi      = {10.1038/s41586-023-06096-3},
  file     = {:papers/Kim2023.pdf:PDF},
  url      = {https://doi.org/10.1038/s41586-023-06096-3},
}

@Article{Acharya2023,
  author   = {Acharya, Rajeev and Aleiner, Igor and Allen, Richard and Andersen, Trond I. and Ansmann, Markus and Arute, Frank and Arya, Kunal and Asfaw, Abraham and Atalaya, Juan and Babbush, Ryan and Bacon, Dave and Bardin, Joseph C. and Basso, Joao and Bengtsson, Andreas and Boixo, Sergio and Bortoli, Gina and Bourassa, Alexandre and Bovaird, Jenna and Brill, Leon and Broughton, Michael and Buckley, Bob B. and Buell, David A. and Burger, Tim and Burkett, Brian and Bushnell, Nicholas and Chen, Yu and Chen, Zijun and Chiaro, Ben and Cogan, Josh and Collins, Roberto and Conner, Paul and Courtney, William and Crook, Alexander L. and Curtin, Ben and Debroy, Dripto M. and Del Toro Barba, Alexander and Demura, Sean and Dunsworth, Andrew and Eppens, Daniel and Erickson, Catherine and Faoro, Lara and Farhi, Edward and Fatemi, Reza and Flores Burgos, Leslie and Forati, Ebrahim and Fowler, Austin G. and Foxen, Brooks and Giang, William and Gidney, Craig and Gilboa, Dar and Giustina, Marissa and Grajales Dau, Alejandro and Gross, Jonathan A. and Habegger, Steve and Hamilton, Michael C. and Harrigan, Matthew P. and Harrington, Sean D. and Higgott, Oscar and Hilton, Jeremy and Hoffmann, Markus and Hong, Sabrina and Huang, Trent and Huff, Ashley and Huggins, William J. and Ioffe, Lev B. and Isakov, Sergei V. and Iveland, Justin and Jeffrey, Evan and Jiang, Zhang and Jones, Cody and Juhas, Pavol and Kafri, Dvir and Kechedzhi, Kostyantyn and Kelly, Julian and Khattar, Tanuj and Khezri, Mostafa and Kieferová, Mária and Kim, Seon and Kitaev, Alexei and Klimov, Paul V. and Klots, Andrey R. and Korotkov, Alexander N. and Kostritsa, Fedor and Kreikebaum, John Mark and Landhuis, David and Laptev, Pavel and Lau, Kim-Ming and Laws, Lily and Lee, Joonho and Lee, Kenny and Lester, Brian J. and Lill, Alexander and Liu, Wayne and Locharla, Aditya and Lucero, Erik and Malone, Fionn D. and Marshall, Jeffrey and Martin, Orion and McClean, Jarrod R. and McCourt, Trevor and McEwen, Matt and Megrant, Anthony and Meurer Costa, Bernardo and Mi, Xiao and Miao, Kevin C. and Mohseni, Masoud and Montazeri, Shirin and Morvan, Alexis and Mount, Emily and Mruczkiewicz, Wojciech and Naaman, Ofer and Neeley, Matthew and Neill, Charles and Nersisyan, Ani and Neven, Hartmut and Newman, Michael and Ng, Jiun How and Nguyen, Anthony and Nguyen, Murray and Niu, Murphy Yuezhen and O’Brien, Thomas E. and Opremcak, Alex and Platt, John and Petukhov, Andre and Potter, Rebecca and Pryadko, Leonid P. and Quintana, Chris and Roushan, Pedram and Rubin, Nicholas C. and Saei, Negar and Sank, Daniel and Sankaragomathi, Kannan and Satzinger, Kevin J. and Schurkus, Henry F. and Schuster, Christopher and Shearn, Michael J. and Shorter, Aaron and Shvarts, Vladimir and Skruzny, Jindra and Smelyanskiy, Vadim and Smith, W. Clarke and Sterling, George and Strain, Doug and Szalay, Marco and Torres, Alfredo and Vidal, Guifre and Villalonga, Benjamin and Vollgraff Heidweiller, Catherine and White, Theodore and Xing, Cheng and Yao, Z. Jamie and Yeh, Ping and Yoo, Juhwan and Young, Grayson and Zalcman, Adam and Zhang, Yaxing and Zhu, Ningfeng and Google Quantum, A. I.},
  journal  = {Nature},
  title    = {Suppressing quantum errors by scaling a surface code logical qubit},
  year     = {2023},
  number   = {7949},
  pages    = {676--681},
  volume   = {614},
  doi      = {10.1038/s41586-022-05434-1},
  file     = {:papers/Acharya2023.pdf:PDF},
  url      = {https://doi.org/10.1038/s41586-022-05434-1},
}

@Article{Huang2021,
  author  = {Hsin-Yuan Huang and R. Kueng and J. Preskill},
  journal = {Physical review letters},
  title   = {Information theoretic bounds on quantum advantage in machine learning},
  year    = {2021},
  pages   = {190505},
  volume  = {126 19},
  doi     = {10.1103/PhysRevLett.126.190505},
}


@Article{Castelvecchi2023,
  author   = {Castelvecchi, Davide},
  journal  = {Nature},
  title    = {IBM QUANTUM COMPUTER PASSES CALCULATION MILESTONE},
  year     = {2023},
  month    = {6},
  pages    = {656--657},
  volume   = {618},
  doi      = {10.1038/s41586-022-05434-1},
  file     = {:papers/Acharya2023.pdf:PDF},
  url      = {https://doi.org/10.1038/s41586-022-05434-1},
}

@InProceedings{Chen2022,
  author    = {Chen, Samuel Yen-Chi and Yoo, Shinjae and Fang, Yao-Lung L.},
  booktitle = {ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {Quantum Long Short-Term Memory},
  year      = {2022},
  pages     = {8622-8626},
  doi       = {10.1109/ICASSP43922.2022.9747369},
  file      = {:papers/Chen2022.pdf:PDF},
  groups    = {QLSTM Experiments},
}

@misc{IBMQa,
author    = {{IBM Quantum}},
title     = {{Now entering the era of quantum utility}},
url       = {https://www.ibm.com/quantum},
year      = {2023},
urldate   = {2023-09-29}
}

@misc{Qiskita,
author    = {{Qiskit Dokumentation: TorchConnector}},
title     = {{TorchConnector}},
url       = {https://qiskit.org/ecosystem/machine-learning/stubs/qiskit_machine_learning.connectors.TorchConnector.html},
year      = {2023},
urldate   = {2023-10-11}
}

@misc{TFa,
author    = {{TensorFlow Quantum Dokumentation}},
title     = {{TensorFlow Quantum: GitHub Repo}},
url       = {https://github.com/tensorflow/quantum},
year      = {2023},
urldate   = {2023-10-11}
}


@misc{GoogleQa,
author    = {{Google Quantum AI}},
title     = {{Explore the possibilities of quantum}},
url       = {https://quantumai.google/},
year      = {2023},
urldate   = {2023-09-29}
}

@misc{DWaveQ,
author    = {{D-Wave}},
title     = {{Unlock the Power of Practical Quantum Computing Today}},
url       = {https://www.dwavesys.com/},
year      = {2023},
urldate   = {2023-09-29}
}

@misc{MicrosoftQ,
author    = {{Microsoft Azure}},
title     = {{Azure Quantum}},
url       = {https://azure.microsoft.com/de-de/solutions/quantum-computing},
year      = {2023},
urldate   = {2023-09-29}
}

@Article{Yu2023,
  author  = {Yu, Yunjun and Hu, Guoping and Liu, Caicheng and Xiong, Junjie and Wu, Ziyang},
  journal = {IEEE Transactions on Quantum Engineering},
  title   = {Prediction of Solar Irradiance One Hour Ahead Based on Quantum Long Short-Term Memory Network},
  year    = {2023},
  pages   = {1-15},
  volume  = {4},
  doi     = {10.1109/TQE.2023.3271362},
  file    = {:papers/Yu2023.pdf:PDF},
  groups  = {QLSTM Experiments},
}

@Misc{Qi2021,
  author        = {Jun Qi and Chao-Han Huck Yang and Pin-Yu Chen},
  title         = {QTN-VQC: An End-to-End Learning framework for Quantum Neural Networks},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2110.03861},
  file          = {:papers/Qi2021.pdf:PDF},
  groups        = {QLSTM Experiments},
  primaryclass  = {quant-ph},
}

@Article{Sim2019,
  author   = {Sim, Sukin and Johnson, Peter D. and Aspuru-Guzik, Alán},
  journal  = {Advanced Quantum Technologies},
  title    = {Expressibility and Entangling Capability of Parameterized Quantum Circuits for Hybrid Quantum-Classical Algorithms},
  year     = {2019},
  number   = {12},
  pages    = {1900070},
  volume   = {2},
  abstract = {Abstract Parameterized quantum circuits (PQCs) play an essential role in the performance of many variational quantum algorithms. One challenge in implementing such algorithms is choosing an effective circuit that well represents the solution space while maintaining a low circuit depth and parameter count. To characterize and identify expressible, yet compact, circuits, several descriptors are proposed, including expressibility and entangling capability, that are statistically estimated from classical simulations. These descriptors are computed for different circuit structures, varying the qubit connectivity and selection of gates. From these simulations, circuit fragments that perform well with respect to the descriptors are identified. In particular, a substantial improvement in performance of two-qubit gates in a ring or all-to-all connected arrangement, compared to that of those on a line, is observed. Furthermore, improvement in both descriptors is achieved by sequences of controlled X-rotation gates compared to sequences of controlled Z-rotation gates. In addition, it is investigated how expressibility “saturates” with increased circuit depth, finding that the rate and saturated value appear to be distinguishing features of a PQC. While the correlation between each descriptor and algorithm performance remains to be investigated, methods and results from this study can be useful for algorithm development and design of experiments.},
  doi      = {https://doi.org/10.1002/qute.201900070},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/qute.201900070},
  file     = {:papers/Sim2019.pdf:PDF},
  groups   = {QLSTM Experiments},
  keywords = {quantum algorithms, quantum circuits, quantum computation},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/qute.201900070},
}

@InProceedings{Vaswani2017,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \L{}ukasz and Polosukhin, Illia},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  title     = {Attention is All You Need},
  year      = {2017},
  address   = {Red Hook, NY, USA},
  pages     = {6000–6010},
  publisher = {Curran Associates Inc.},
  series    = {NIPS'17},
  abstract  = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
  isbn      = {9781510860964},
  location  = {Long Beach, California, USA},
  numpages  = {11},
}

@InProceedings{Venugopalan2015,
  author    = {Venugopalan, Subhashini and Rohrbach, Marcus and Donahue, Jeffrey and Mooney, Raymond and Darrell, Trevor and Saenko, Kate},
  booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
  title     = {Sequence to Sequence -- Video to Text},
  year      = {2015},
  pages     = {4534-4542},
  doi       = {10.1109/ICCV.2015.515},
}

@InProceedings{Sutskever2014,
  author    = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
  title     = {Sequence to Sequence Learning with Neural Networks},
  year      = {2014},
  address   = {Cambridge, MA, USA},
  pages     = {3104–3112},
  publisher = {MIT Press},
  series    = {NIPS'14},
  abstract  = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  location  = {Montreal, Canada},
  numpages  = {9},
}

@Article{Webster2002,
  author    = {Webster, Jane and Watson, Richard T},
  journal   = {MIS quarterly},
  title     = {Analyzing the past to prepare for the future: Writing a literature review},
  year      = {2002},
  pages     = {xiii--xxiii},
  publisher = {JSTOR},
}

@Article{VanHoudt2020,
  author   = {Van Houdt, Greg and Mosquera, Carlos and Nápoles, Gonzalo},
  journal  = {Artificial Intelligence Review},
  title    = {A review on the long short-term memory model},
  year     = {2020},
  issn     = {1573-7462},
  number   = {8},
  pages    = {5929--5955},
  volume   = {53},
  abstract = {Long short-term memory (LSTM) has transformed both machine learning and neurocomputing fields. According to several online sources, this model has improved Google’s speech recognition, greatly improved machine translations on Google Translate, and the answers of Amazon’s Alexa. This neural system is also employed by Facebook, reaching over 4 billion LSTM-based translations per day as of 2017. Interestingly, recurrent neural networks had shown a rather discrete performance until LSTM showed up. One reason for the success of this recurrent network lies in its ability to handle the exploding/vanishing gradient problem, which stands as a difficult issue to be circumvented when training recurrent or very deep neural networks. In this paper, we present a comprehensive review that covers LSTM’s formulation and training, relevant applications reported in the literature and code resources implementing this model for a toy example.},
  doi      = {10.1007/s10462-020-09838-1},
  file     = {:papers/VanHoudt2020.pdf:PDF},
  groups   = {Klassische LSTM},
  refid    = {Van Houdt2020},
  url      = {https://doi.org/10.1007/s10462-020-09838-1},
}

@InProceedings{Sak2014,
  author    = {Hasim Sak and Andrew W. Senior and Françoise Beaufays},
  booktitle = {INTERSPEECH},
  title     = {Long short-term memory recurrent neural network architectures for large scale acoustic modeling},
  year      = {2014},
  pages     = {338-342},
  file      = {:papers/Sak2014.pdf:PDF},
  groups    = {Klassische LSTM},
}

@Article{Hochreiter1997,
  author  = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  journal = {Neural Computation},
  title   = {Long Short-Term Memory},
  year    = {1997},
  number  = {8},
  pages   = {1735-1780},
  volume  = {9},
  doi     = {10.1162/neco.1997.9.8.1735},
  file    = {:papers/Hochreiter1997.pdf:PDF},
  groups  = {Klassische LSTM},
}

@Article{Cao2023,
  author  = {Cao, Yuji and Zhou, Xiyuan and Fei, Xiang and Zhao, Huan and Liu, Wenxuan and Zhao, Junhua},
  journal = {Quantum Machine Intelligence},
  title   = {Linear-layer-enhanced quantum long short-term memory for carbon price forecasting},
  year    = {2023},
  month   = {07},
  volume  = {5},
  doi     = {10.1007/s42484-023-00115-2},
  file    = {:papers/Cao2023.pdf:PDF},
  groups  = {QLSTM Experiments},
}

@Article{Sagheer2019,
  author   = {Alaa Sagheer and Mostafa Kotb},
  journal  = {Neurocomputing},
  title    = {Time series forecasting of petroleum production using deep LSTM recurrent networks},
  year     = {2019},
  issn     = {0925-2312},
  pages    = {203-213},
  volume   = {323},
  abstract = {Time series forecasting (TSF) is the task of predicting future values of a given sequence using historical data. Recently, this task has attracted the attention of researchers in the area of machine learning to address the limitations of traditional forecasting methods, which are time-consuming and full of complexity. With the increasing availability of extensive amounts of historical data along with the need of performing accurate production forecasting, particularly a powerful forecasting technique infers the stochastic dependency between past and future values is highly needed. In this paper, we propose a deep learning approach capable to address the limitations of traditional forecasting approaches and show accurate predictions. The proposed approach is a deep long-short term memory (DLSTM) architecture, as an extension of the traditional recurrent neural network. Genetic algorithm is applied in order to optimally configure DLSTM’s optimum architecture. For evaluation purpose, two case studies from the petroleum industry domain are carried out using the production data of two actual oilfields. Toward a fair evaluation, the performance of the proposed approach is compared with several standard methods, either statistical or soft computing. Using different measurement criteria, the empirical results show that the proposed DLSTM model outperforms other standard approaches.},
  doi      = {https://doi.org/10.1016/j.neucom.2018.09.082},
  file     = {:papers/Sagheer2019.pdf:PDF},
  groups   = {Klassische LSTM},
  keywords = {Time series forecasting, Deep neural networks, Recurrent neural networks, Long-short term memory, Petroleum production forecasting},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231218311639},
}

@Article{Liu2019,
  author   = {Yang Liu},
  journal  = {Expert Systems with Applications},
  title    = {Novel volatility forecasting using deep learning–Long Short Term Memory Recurrent Neural Networks},
  year     = {2019},
  issn     = {0957-4174},
  pages    = {99-109},
  volume   = {132},
  abstract = {Abstracts
The volatility is related to financial risk and its prediction accuracy is very important in portfolio optimisation. A large body of literature to-date suggests Support Vector Machines (SVM) as the “best of regression” algorithms for financial data regression. Recent work however found that new deep learning––Long Short Term Memory Recurrent Neural Networks (LSTM RNNs) outperformed SVM for classification problems. In the present paper we conduct a new unbiased evaluation of these two modelling techniques for regression problems, and we also compare them with a popular regression model - Generalized Autoregressive Conditional Heteroskedasticity (GARCH) model for financial volatility or risk forecasting. Our experiments using financial data show that the LSTM RNNs performed as good as v-SVR for large interval volatility forecasting and both performed much better than GARCH model for two financial indices (S&P 500 and AAPL). The LSTM RNNS deep learning method can learn from big raw data and can be run with many hidden layers and neurons under GPU to achieve a good prediction for long sequence data compared to the support vector regression. The deep learning technique - LSTM RNNs with big data can be used to improve the volatility prediction instead of v-SVR when the v-SVR does not predict well for some financial stocks of a portfolio. This will help investors to win the competition to maximize their profit.},
  doi      = {https://doi.org/10.1016/j.eswa.2019.04.038},
  groups   = {Klassische LSTM},
  keywords = {Deep learning, Long Short Term Memory Recurrent Neural Networks, Support Vector Machines (SVM), Generalized Autoregressive Conditional Heteroskedasticity (GARCH) model, Volatility forecasting},
  url      = {https://www.sciencedirect.com/science/article/pii/S0957417419302635},
}

@Article{Fischer2018,
  author   = {Thomas Fischer and Christopher Krauss},
  journal  = {European Journal of Operational Research},
  title    = {Deep learning with long short-term memory networks for financial market predictions},
  year     = {2018},
  issn     = {0377-2217},
  number   = {2},
  pages    = {654-669},
  volume   = {270},
  abstract = {Long short-term memory (LSTM) networks are a state-of-the-art technique for sequence learning. They are less commonly applied to financial time series predictions, yet inherently suitable for this domain. We deploy LSTM networks for predicting out-of-sample directional movements for the constituent stocks of the S&P 500 from 1992 until 2015. With daily returns of 0.46 percent and a Sharpe ratio of 5.8 prior to transaction costs, we find LSTM networks to outperform memory-free classification methods, i.e., a random forest (RAF), a deep neural net (DNN), and a logistic regression classifier (LOG). The outperformance relative to the general market is very clear from 1992 to 2009, but as of 2010, excess returns seem to have been arbitraged away with LSTM profitability fluctuating around zero after transaction costs. We further unveil sources of profitability, thereby shedding light into the black box of artificial neural networks. Specifically, we find one common pattern among the stocks selected for trading – they exhibit high volatility and a short-term reversal return profile. Leveraging these findings, we are able to formalize a rules-based short-term reversal strategy that yields 0.23 percent prior to transaction costs. Further regression analysis unveils low exposure of the LSTM returns to common sources of systematic risk – also compared to the three benchmark models.},
  doi      = {https://doi.org/10.1016/j.ejor.2017.11.054},
  groups   = {Klassische LSTM},
  keywords = {Finance, Statistical arbitrage, LSTM, Machine learning, Deep learning},
  ranking  = {rank5},
  url      = {https://www.sciencedirect.com/science/article/pii/S0377221717310652},
}

@Article{Yan2018,
  author   = {Yan, Hongju and Ouyang, Hongbing},
  journal  = {Wireless Personal Communications},
  title    = {Financial Time Series Prediction Based on Deep Learning},
  year     = {2018},
  issn     = {1572-834X},
  number   = {2},
  pages    = {683--700},
  volume   = {102},
  abstract = {By combining wavelet analysis with Long Short-Term Memory (LSTM) neural network, this paper proposes a time series prediction model to capture the complex features such as non-linearity, non-stationary and sequence correlation of financial time series. The LSTM is then applied to the prediction of the daily closing price of the Shanghai Composite Index as well as the comparison of its prediction ability with machine learning models such as multi-layer perceptron, support vector machine and K-nearest neighbors. The empirical results show that the LSTM performs a better prediction effect, and it shows excellent effects on the static prediction and dynamic trend prediction of the financial time series, which indicates its applicability and effectiveness to the prediction of financial time series. At the same time, both wavelet decomposition and reconstruction of financial time series can improve the generalization ability of the LSTM prediction model and the prediction accuracy of long-term dynamic trend.},
  doi      = {10.1007/s11277-017-5086-2},
  file     = {:papers/Yan2018.pdf:PDF},
  groups   = {Klassische LSTM},
  refid    = {Yan2018},
  url      = {https://doi.org/10.1007/s11277-017-5086-2},
}

@Article{Rodrigues2019,
  author   = {Filipe Rodrigues and Ioulia Markou and Francisco C. Pereira},
  journal  = {Information Fusion},
  title    = {Combining time-series and textual data for taxi demand prediction in event areas: A deep learning approach},
  year     = {2019},
  issn     = {1566-2535},
  pages    = {120-129},
  volume   = {49},
  abstract = {Accurate time-series forecasting is vital for numerous areas of application such as transportation, energy, finance, economics, etc. However, while modern techniques are able to explore large sets of temporal data to build forecasting models, they typically neglect valuable information that is often available under the form of unstructured text. Although this data is in a radically different format, it often contains contextual explanations for many of the patterns that are observed in the temporal data. In this paper, we propose two deep learning architectures that leverage word embeddings, convolutional layers and attention mechanisms for combining text information with time-series data. We apply these approaches for the problem of taxi demand forecasting in event areas. Using publicly available taxi data from New York, we empirically show that by fusing these two complementary cross-modal sources of information, the proposed models are able to significantly reduce the error in the forecasts.},
  doi      = {https://doi.org/10.1016/j.inffus.2018.07.007},
  groups   = {Klassische LSTM},
  keywords = {Deep learning, Data fusion, Cross modality learning, Time series forecasting, Textual data, Taxi demand, Special events, Urban mobility},
  url      = {https://www.sciencedirect.com/science/article/pii/S1566253517308175},
}

@Article{Gers2000,
  author     = {Gers, Felix A. and Schmidhuber, J\"{u}rgen A. and Cummins, Fred A.},
  journal    = {Neural Comput.},
  title      = {Learning to Forget: Continual Prediction with LSTM},
  year       = {2000},
  issn       = {0899-7667},
  month      = {oct},
  number     = {10},
  pages      = {2451–2471},
  volume     = {12},
  abstract   = {Long short-term memory (LSTM; Hochreiter \& Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset. Without resets, the state may grow indefinitely and eventually cause the network to break down. Our remedy is a novel, adaptive "forget gate" that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustrative benchmark problems on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve continual versions of these problems. LSTM with forget gates, however, easily solves them, and in an elegant way.},
  address    = {Cambridge, MA, USA},
  doi        = {10.1162/089976600300015015},
  groups     = {Klassische LSTM},
  issue_date = {October 2000},
  numpages   = {21},
  publisher  = {MIT Press},
  url        = {https://doi.org/10.1162/089976600300015015},
}

@Article{Informatik2003,
  author  = {Bengio, Y. and Frasconi, Paolo and Schmidhuber, Jfirgen},
  journal = {A Field Guide to Dynamical Recurrent Neural Networks},
  title   = {Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies},
  year    = {2003},
  month   = {03},
  groups  = {Klassische LSTM},
}

@InBook{Panesar2021,
  author    = {Panesar, Arjun},
  pages     = {189--205},
  publisher = {Apress},
  title     = {Evaluating Machine Learning Models},
  year      = {2021},
  address   = {Berkeley, CA},
  isbn      = {978-1-4842-6537-6},
  abstract  = {As a field, machine learning is still in its infancy. Advanced machine learning has only been explored over the last 25 years, which has fueled data science as a profession. Thus, the data science industry is still in a phase of wonderment at the endless potential of AI and machine learning. With this come both excitement and confusion---and an industry that is gathering knowledge, experience, and first-time problems.},
  booktitle = {Machine Learning and AI for Healthcare : Big Data for Improved Health Outcomes},
  doi       = {10.1007/978-1-4842-6537-6_7},
  file      = {:papers/Panesar2021.pdf:PDF},
  groups    = {ml models},
  url       = {https://doi.org/10.1007/978-1-4842-6537-6_7},
}

@Article{Werbos1990,
  author  = {Werbos, P.J.},
  journal = {Proceedings of the IEEE},
  title   = {Backpropagation through time: what it does and how to do it},
  year    = {1990},
  number  = {10},
  pages   = {1550-1560},
  volume  = {78},
  doi     = {10.1109/5.58337},
  groups  = {ml models},
}

@Article{Kingma2014,
  author  = {Kingma, Diederik and Ba, Jimmy},
  journal = {International Conference on Learning Representations},
  title   = {Adam: A Method for Stochastic Optimization},
  year    = {2014},
  month   = dec,
  groups  = {ml models},
}

@Article{Biamonte2017,
  author   = {Biamonte, Jacob and Wittek, Peter and Pancotti, Nicola and Rebentrost, Patrick and Wiebe, Nathan and Lloyd, Seth},
  journal  = {Nature},
  title    = {Quantum machine learning},
  year     = {2017},
  issn     = {1476-4687},
  number   = {7671},
  pages    = {195--202},
  volume   = {549},
  abstract = {Fuelled by increasing computer power and algorithmic advances, machine learning techniques have become powerful tools for finding patterns in data. Quantum systems produce atypical patterns that classical systems are thought not to produce efficiently, so it is reasonable to postulate that quantum computers may outperform classical computers on machine learning tasks. The field of quantum machine learning explores how to devise and implement quantum software that could enable machine learning that is faster than that of classical computers. Recent work has produced quantum algorithms that could act as the building blocks of machine learning programs, but the hardware and software challenges are still considerable.},
  doi      = {10.1038/nature23474},
  file     = {:papers/Biamonte2017.pdf:PDF},
  groups   = {qml circuits},
  refid    = {Biamonte2017},
  url      = {https://doi.org/10.1038/nature23474},
}

@Article{Monaco2023,
  author    = {Monaco, Saverio and Kiss, Oriel and Mandarino, Antonio and Vallecorsa, Sofia and Grossi, Michele},
  journal   = {Phys. Rev. B},
  title     = {Quantum phase detection generalization from marginal quantum neural network models},
  year      = {2023},
  month     = {Feb},
  pages     = {L081105},
  volume    = {107},
  doi       = {10.1103/PhysRevB.107.L081105},
  file      = {:papers/Monaco2023.pdf:PDF},
  groups    = {qml circuits},
  issue     = {8},
  numpages  = {6},
  publisher = {American Physical Society},
  url       = {https://link.aps.org/doi/10.1103/PhysRevB.107.L081105},
}

@Article{McClean2018,
  author   = {McClean, Jarrod R. and Boixo, Sergio and Smelyanskiy, Vadim N. and Babbush, Ryan and Neven, Hartmut},
  journal  = {Nature Communications},
  title    = {Barren plateaus in quantum neural network training landscapes},
  year     = {2018},
  issn     = {2041-1723},
  number   = {1},
  pages    = {4812},
  volume   = {9},
  abstract = {Many experimental proposals for noisy intermediate scale quantum devices involve training a parameterized quantum circuit with a classical optimization loop. Such hybrid quantum-classical algorithms are popular for applications in quantum simulation, optimization, and machine learning. Due to its simplicity and hardware efficiency, random circuits are often proposed as initial guesses for exploring the space of quantum states. We show that the exponential dimension of Hilbert space and the gradient estimation complexity make this choice unsuitable for hybrid quantum-classical algorithms run on more than a few qubits. Specifically, we show that for a wide class of reasonable parameterized quantum circuits, the probability that the gradient along any reasonable direction is non-zero to some fixed precision is exponentially small as a function of the number of qubits. We argue that this is related to the 2-design characteristic of random circuits, and that solutions to this problem must be studied.},
  doi      = {10.1038/s41467-018-07090-4},
  file     = {:papers/McClean2018.pdf:PDF},
  groups   = {qml circuits},
  refid    = {McClean2018},
  url      = {https://doi.org/10.1038/s41467-018-07090-4},
}

@InProceedings{Hu2022,
  author    = {Hu, Zhirui and Dong, Peiyan and Wang, Zhepeng and Lin, Youzuo and Wang, Yanzhi and Jiang, Weiwen},
  booktitle = {Proceedings of the 41st IEEE/ACM International Conference on Computer-Aided Design},
  title     = {Quantum Neural Network Compression},
  year      = {2022},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ICCAD '22},
  abstract  = {Model compression, such as pruning and quantization, has been widely applied to optimize neural networks on resource-limited classical devices. Recently, there are growing interest in variational quantum circuits (VQC), that is, a type of neural network on quantum computers (a.k.a., quantum neural networks). It is well known that the near-term quantum devices have high noise and limited resources (i.e., quantum bits, qubits); yet, how to compress quantum neural networks has not been thoroughly studied. One might think it is straightforward to apply the classical compression techniques to quantum scenarios. However, this paper reveals that there exist differences between the compression of quantum and classical neural networks. Based on our observations, we claim that the compilation/traspilation has to be involved in the compression process. On top of this, we propose the very first systematical framework, namely CompVQC, to compress quantum neural networks (QNNs). In CompVQC, the key component is a novel compression algorithm, which is based on the alternating direction method of multipliers (ADMM) approach. Experiments demonstrate the advantage of the CompVQC, reducing the circuit depth (almost over 2.5\texttimes{}) with a negligible accuracy drop (<1\%), which outperforms other competitors. Another promising truth is our CompVQC can indeed promote the robustness of the QNN on the near-term noisy quantum devices.},
  articleno = {140},
  doi       = {10.1145/3508352.3549382},
  file      = {:papers/Hu2022.pdf:PDF},
  groups    = {qml circuits},
  isbn      = {9781450392174},
  location  = {San Diego, California},
  numpages  = {9},
  url       = {https://doi.org/10.1145/3508352.3549382},
}

@Misc{Qiskitb,
  author  = {{Qiskit Textbook: QNN}},
  title   = {{Quantum Neural Networks}},
  url     = {https://qiskit.org/ecosystem/machine-learning/tutorials/01_neural_networks.html},
  urldate = {2023-10-11},
  year    = {2023},
  groups  = {qml circuits},
}

@Book{Williams1998,
  author    = {Williams, Colin P and Clearwater, Scott H and others},
  publisher = {Springer},
  title     = {Explorations in quantum computing},
  year      = {1998},
  file      = {:papers/Williams1998.pdf:PDF},
  groups    = {qml circuits},
}

@Article{Mitarai2018,
  author    = {Mitarai, K. and Negoro, M. and Kitagawa, M. and Fujii, K.},
  journal   = {Phys. Rev. A},
  title     = {Quantum circuit learning},
  year      = {2018},
  month     = {Sep},
  pages     = {032309},
  volume    = {98},
  doi       = {10.1103/PhysRevA.98.032309},
  file      = {:papers/Mitarai2018.pdf:PDF},
  groups    = {qml circuits},
  issue     = {3},
  numpages  = {6},
  publisher = {American Physical Society},
  url       = {https://link.aps.org/doi/10.1103/PhysRevA.98.032309},
}

@InProceedings{Kwak2021,
  author    = {Kwak, Yunseok and Yun, Won Joon and Jung, Soyi and Kim, Joongheon},
  booktitle = {2021 Twelfth International Conference on Ubiquitous and Future Networks (ICUFN)},
  title     = {Quantum Neural Networks: Concepts, Applications, and Challenges},
  year      = {2021},
  pages     = {413-416},
  doi       = {10.1109/ICUFN49451.2021.9528698},
  file      = {:papers/Kwak2021.pdf:PDF},
  groups    = {qml circuits},
}

@Article{OvalleMagallanes2023,
  author   = {Emmanuel Ovalle-Magallanes and Dora E. Alvarado-Carrillo and Juan Gabriel Avina-Cervantes and Ivan Cruz-Aceves and Jose Ruiz-Pinales},
  journal  = {Applied Soft Computing},
  title    = {Quantum angle encoding with learnable rotation applied to quantum–classical convolutional neural networks},
  year     = {2023},
  issn     = {1568-4946},
  pages    = {110307},
  volume   = {141},
  abstract = {Quantum Machine Learning (QML) has experienced rapid progress in recent years due to the development of Noisy Intermediate-Scale Quantum (NISQ) devices and quantum simulators. Two key elements must be minimized To maintain acceptable computational complexity in QML: the number of qubits required to encode classical data and the number of quantum gates. This paper proposes a novel angle encoding with learnable rotation to drastically reduce the qubits and circuit depth from O(N) to O(⌈log2(N)⌉) qubits, and only N parameterized gates, where N is the input size. Additionally, an extended quantum convolutional layer is introduced with multiple quantum circuits (quantum kernel) that allow for the configuration of any arbitrary size, stride, and dilation analogous to a classical convolutional layer. The proposed quantum convolutional layer learns multiple feature maps with a single quantum kernel while reducing computational cost by employing angle encoding with learnable rotation. Extensive experiments were performed by comparing diverse types of quantum convolutional configurations in a Quantum Convolutional Neural Network (QCNN) over a balanced subset of the MNIST and Fashion-MNIST datasets, achieving an accuracy of 0.90 and 0.7850, respectively.},
  doi      = {https://doi.org/10.1016/j.asoc.2023.110307},
  file     = {:papers/OvalleMagallanes2023.pdf:PDF},
  groups   = {Data Encoding (Quantum)},
  keywords = {Quantum angle encoding, Quantum–classical neural networks, Variational quantum circuits},
  url      = {https://www.sciencedirect.com/science/article/pii/S1568494623003253},
}

@Article{Liu2021,
  author   = {Liu, Junhua and Lim, Kwan Hui and Wood, Kristin L. and Huang, Wei and Guo, Chu and Huang, He-Liang},
  journal  = {Science Chine Physics, Mechanics and Astronomy},
  title    = {Hybrid quantum-classical convolutional neural networks},
  year     = {2021},
  issn     = {1869-1927},
  number   = {9},
  pages    = {290311},
  volume   = {64},
  abstract = {Deep learning has been shown to be able to recognize data patterns better than humans in specific circumstances or contexts. In parallel, quantum computing has demonstrated to be able to output complex wave functions with a few number of gate operations, which could generate distributions that are hard for a classical computer to produce. Here we propose a hybrid quantum-classical convolutional neural network (QCCNN), inspired by convolutional neural networks (CNNs) but adapted to quantum computing to enhance the feature mapping process. QCCNN is friendly to currently noisy intermediate-scale quantum computers, in terms of both number of qubits as well as circuit’s depths, while retaining important features of classical CNN, such as nonlinearity and scalability. We also present a framework to automatically compute the gradients of hybrid quantum-classical loss functions which could be directly applied to other hybrid quantum-classical algorithms. We demonstrate the potential of this architecture by applying it to a Tetris dataset, and show that QCCNN can accomplish classification tasks with learning accuracy surpassing that of classical CNN with the same structure.},
  doi      = {10.1007/s11433-021-1734-3},
  groups   = {Data Encoding (Quantum)},
  refid    = {Liu2021},
  url      = {https://doi.org/10.1007/s11433-021-1734-3},
}

@Article{Henderson2020,
  author   = {Henderson, Maxwell and Shakya, Samriddhi and Pradhan, Shashindra and Cook, Tristan},
  journal  = {Quantum Machine Intelligence},
  title    = {Quanvolutional neural networks: powering image recognition with quantum circuits},
  year     = {2020},
  issn     = {2524-4914},
  number   = {1},
  pages    = {2},
  volume   = {2},
  abstract = {Convolutional neural networks (CNNs) have rapidly risen in popularity for many machine learning applications, particularly in the field of image recognition. Much of the benefit generated from these networks comes from their ability to extract features from the data in a hierarchical manner. These features are extracted using various transformational layers, notably the convolutional layer which gives the model its name. In this work, we introduce a new type of transformational layer called a quantum convolution, or quanvolutional layer. Quanvolutional layers operate on input data by locally transforming the data using a number of random quantum circuits, in a way that is similar to the transformations performed by random convolutional filter layers. Provided these quantum transformations produce meaningful features for classification purposes, then this algorithm could be of practical use for near-term quantum computers as it requires small quantum circuits with little to no error correction. In this work, we empirically evaluated the potential benefit of these quantum transformations by comparing three types of models built on the MNIST dataset: CNNs, quantum convolutional neural networks (QNNs), and CNNs with additional non-linearities introduced. Our results showed that the QNN models had both higher test set accuracy as well as faster training compared with the purely classical CNNs.},
  doi      = {10.1007/s42484-020-00012-y},
  groups   = {Data Encoding (Quantum)},
  ranking  = {rank3},
  refid    = {Henderson2020},
  url      = {https://doi.org/10.1007/s42484-020-00012-y},
}

@Article{Mari2020,
  author  = {Mari, Andrea and Bromley, Thomas R. and Izaac, Josh and Schuld, Maria and Killoran, Nathan},
  journal = {{Quantum}},
  title   = {Transfer learning in hybrid classical-quantum neural networks},
  year    = {2020},
  month   = {10},
  pages   = {340},
  volume  = {4},
  doi     = {10.22331/q-2020-10-09-340},
  groups  = {Data Encoding (Quantum)},
  url     = {https://doi.org/10.22331/q-2020-10-09-340},
}

@Book{Busch2016,
  author    = {Busch, Paul and Lahti, Pekka and Pellonpää, Juha-Pekka and Ylinen, Kari},
  publisher = {Springer},
  title     = {{Quantum measurement}},
  year      = {2016},
  month     = {8},
  groups    = {Data Encoding (Quantum)},
}

@Book{Braginsky1995,
  author    = {Braginsky, Vladimir B. and Braginskiĭ, Vladimir Borisovich and Khalili, Farid Ya},
  publisher = {Cambridge University Press},
  title     = {{Quantum measurement}},
  year      = {1995},
  month     = {5},
  groups    = {Data Encoding (Quantum)},
}

@Book{Wiseman2010,
  author    = {Wiseman, Howard M. and Milburn, Gerard J.},
  publisher = {Cambridge University Press},
  title     = {{Quantum measurement and control}},
  year      = {2010},
  month     = {1},
  groups    = {Data Encoding (Quantum)},
}

@InProceedings{Chen2023,
  author    = {Chen, Samuel Yen-Chi},
  booktitle = {ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {Quantum Deep Recurrent Reinforcement Learning},
  year      = {2023},
  pages     = {1-5},
  doi       = {10.1109/ICASSP49357.2023.10096981},
  file      = {:papers/Chen2023.pdf:PDF},
  groups    = {QLSTM Experiments},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Data Encoding (Quantum)\;0\;1\;0x8a8a8aff\;DATABASE_ARROW_LEFT\;\;;
1 StaticGroup:Klassische LSTM\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:QLSTM Experiments\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:ml models\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:qml circuits\;0\;1\;0x8a8a8aff\;\;\;;
}

@Comment{jabref-entrytype: online: req[url;urldate] opt[]}
