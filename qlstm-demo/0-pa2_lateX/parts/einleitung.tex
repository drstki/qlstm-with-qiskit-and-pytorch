\chapter{Einleitung}

Für die Verarbeitung von sequentiellen Daten, werden in der Praxis \ac{LSTM}-Modelle eingesetzt.\footcite[Vgl.][]{Sutskever2014} Beispielhaft hierfür sind die Anwendung bei Video zu Text Applikationen\footcite[Vgl.][]{Venugopalan2015} oder Timeseries Forecasting (Zeitreihenvorhersagung)\footcite[Vgl.][]{Chen2022}.
LSTM Architekturen sind außerdem die Basis der für GPT-3 benutzten Transformer Modelle.\footcite[Vgl.][S. 1ff.]{Vaswani2017}
In der aktuellen Literatur stellen \cite{Chen2022} und \cite{Yu2023} jeweils \ac{LSTM}-Architekturen vor, welche mit Quantum Computing erweitert wurden. Diese sogenannten \ac{QLSTM}-Architekturen, überzeugen mit einer höheren Genauigkeit und einer schnelleren Trainingszeit.
Währendessen gibt es immer mehr Unternehmen wie IBM\footcite[Vgl.][]{IBMQa}, Google\footcite[Vgl.][]{GoogleQa}, Microsoft\footcite[Vgl.][]{MicrosoftQ} und D-Wave\footcite[Vgl.][]{DWaveQ}, welche einsatzfähige Quantumcomputer entwickelt haben. Quantenfehlerverhinderung Algorithmen von IBM\footcite[Vgl.][]{Kim2023} und Google\footcite[Vgl.][]{Acharya2023} beschleunigen die Entwicklung zusätzlich.
Ein Quantumfortschritt lässt sich auch bei Machine Learning beobachten,~\cite{Abbas2021} zeigen, dass gewisse \ac{QNN}'s bessere Ergebnisse liefern und dazu schneller trainert werden können, als vergleichbare klassische Modelle.
In diversen physikalischen Experimenten haben \cite{Huang2021} einen aktueller Quantenvorteil im Vergleich zu klassischen Machine Learning Modellen nachgewiesen. 
Bei zyklischen wellenformigen Datensätzen, wie zum Beispiel einer Sinuskurve oder einem gedämpften Oszillator, haben spezielle \ac{QLSTM} Architekturen einen Vorteil gegenüber klassischen Modellen.\footcite[Vgl.][]{Chen2022}
Quantum Machine Learning Modelle eröffnen also perspektivisch die Aussicht, aktuelle hochkomplexe physikalische und chemische Probleme zu lösen.\footcite[Vgl.][S. 1]{Huang2021}

Wie gut ein \ac{QNN} performt hängt stark von der Architektur ab. Die Feature Map und der Variational Layer können beliebig angepasst werden und sind somit mäßgeblich für die Genauigkeit des Modells verantwortlich.\footcite[Vgl.][S. 404]{Abbas2021}
Wie sich verschiedene Feature Maps und Variational Layer auf unterschiedliche Datensätze auswirken ist hierbei noch zu untersuchen.\footcite[Vgl.][S. 407]{Abbas2021}
Aktuell gibt es einige wissenschaftliche Artikel\footcite[Vgl.][]{Chen2022,Yu2023,Qi2021} welche verschienden \ac{QLSTM} Architekturen benutzen um spezifische Datensätze zu bearbeiten. 
Diese Publikationen benutzten alle unterschidliche Feature Maps, Variational Layer und eigene Anwedungsfälle. Keine der Publikationen vergleicht die verschiedenen Architekturen miteinander oder wendet das Modell auf unterschiedliche Datensätze an.

Angesichts dieser beschriebenen Problematik stellt sich die Frage, \textbf{welche Art von \ac{QLSTM}-Architektur für unterschiedliche Datensätze am besten geeignet ist}. Das Ziel dieses Praxisarbeit besteht darin, vier verschiedene \ac{QLSTM}-Architekturen auf zwei unterschiedliche Datensätze anzuwenden, um sie im Anschluss miteinander zu vergleichen und zu evaluieren. 
Hierdurch soll ermittelt werden, ob eine spezifische \ac{QLSTM}-Architektur existiert, die für beide Datensätze als besonders geeignet ist, oder ob bestimmte Architekturen für spezifische Datensätze besser geeignet sind. 
Die Auswahl der Datensätze und Architekturen erfolgt dabei so, dass möglichst alle relevanten Aspekte dieser Fragestellung abgedeckt werden.

Forschungsmethode (Experiment) und Aufbau tbd.